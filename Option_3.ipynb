{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcded49c-acba-44db-ba86-f06ae02beb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Active Learning: IF + EMC + XGBoost ---\n",
    "# Paper reproduction version: N=5 runs, median F1 + 95% CI\n",
    "\n",
    "# Step 1: Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load labeled data\n",
    "labeled_df = pd.read_csv('elliptic_txs_features_labeled.csv')\n",
    "\n",
    "# Check data\n",
    "print(\"Loaded labeled CSV rows:\", labeled_df.shape[0])\n",
    "print(\"Unique time_steps:\", sorted(labeled_df['time_step'].unique()))\n",
    "print(\"Label distribution:\\n\", labeled_df['class'].value_counts())\n",
    "\n",
    "# Step 3: Split train/test by time_step\n",
    "train_df = labeled_df[labeled_df['time_step'] <= 34].reset_index(drop=True)\n",
    "test_df  = labeled_df[labeled_df['time_step'] >= 35].reset_index(drop=True)\n",
    "\n",
    "# Prepare test set\n",
    "X_test = test_df.drop(columns=['txId', 'time_step', 'class'])\n",
    "y_test = (test_df['class'] == 1).astype(int)\n",
    "\n",
    "# Feature columns\n",
    "feature_columns = [col for col in train_df.columns if col.startswith('feature_') or col.startswith('f')]\n",
    "\n",
    "# Config\n",
    "batch_size = 50\n",
    "max_iterations = 30\n",
    "N_runs = 5\n",
    "\n",
    "# Storage for all runs\n",
    "all_f1_scores = []\n",
    "\n",
    "# Step 4: Repeat N=5 runs\n",
    "for run in range(N_runs):\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"=== Run {run+1}/{N_runs}, Random State {run+1}\")\n",
    "    print(\"==========================\")\n",
    "    \n",
    "    # Initialize pools\n",
    "    unlabeled_pool = train_df.copy()\n",
    "    labeled_pool = pd.DataFrame(columns=train_df.columns)\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Warm-up using Isolation Forest\n",
    "    print(\"\\n===== Warm-up stage: Isolation Forest =====\")\n",
    "    iso_forest = IsolationForest(random_state=run+1)\n",
    "    iso_forest.fit(unlabeled_pool[feature_columns])\n",
    "    anomaly_scores = -iso_forest.score_samples(unlabeled_pool[feature_columns])\n",
    "    unlabeled_pool['anomaly_score'] = anomaly_scores\n",
    "    unlabeled_pool = unlabeled_pool.sort_values(by='anomaly_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    found_illicit = False\n",
    "    pointer = 0\n",
    "    warmup_illicit = 0\n",
    "    \n",
    "    while not found_illicit and pointer < len(unlabeled_pool):\n",
    "        batch = unlabeled_pool.iloc[pointer:pointer+batch_size]\n",
    "        labeled_pool = pd.concat([labeled_pool, batch], ignore_index=True)\n",
    "        pointer += batch_size\n",
    "        warmup_illicit = (labeled_pool['class'] == 1).sum()\n",
    "        found_illicit = warmup_illicit > 0\n",
    "    \n",
    "    print(f\"Warm-up selected {len(labeled_pool)} samples.\")\n",
    "    print(f\"Warm-up found {warmup_illicit} illicit samples.\")\n",
    "    \n",
    "    # Remove warm-up samples\n",
    "    unlabeled_pool = unlabeled_pool.iloc[pointer:].drop(columns='anomaly_score').reset_index(drop=True)\n",
    "    \n",
    "    # Active Learning loop\n",
    "    iteration = 1\n",
    "    print(\"\\n===== Active Learning loop: Expected Model Change =====\")\n",
    "    \n",
    "    while len(unlabeled_pool) > 0 and iteration <= max_iterations:\n",
    "        X_train = labeled_pool[feature_columns]\n",
    "        y_train = (labeled_pool['class'] == 1).astype(int)\n",
    "        \n",
    "        xgb_clf = xgb.XGBClassifier(\n",
    "            tree_method='hist',\n",
    "            eval_metric='logloss',\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=run+1\n",
    "        )\n",
    "        \n",
    "        xgb_clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = xgb_clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "        print(f\"[Run {run+1}] XGBoost_Model_{iteration:02d} illicit F1-score: {f1:.4f}\")\n",
    "        \n",
    "        # EMC sampling\n",
    "        X_unlabeled = unlabeled_pool[feature_columns]\n",
    "        proba = xgb_clf.predict_proba(X_unlabeled)[:, 1]\n",
    "        emc_score = proba * (1 - proba)\n",
    "        \n",
    "        select_idx = np.argsort(-emc_score)[:batch_size]\n",
    "        new_batch = unlabeled_pool.iloc[select_idx]\n",
    "        \n",
    "        labeled_pool = pd.concat([labeled_pool, new_batch], ignore_index=True)\n",
    "        unlabeled_pool = unlabeled_pool.drop(unlabeled_pool.index[select_idx]).reset_index(drop=True)\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    # Save F1-scores of this run\n",
    "    all_f1_scores.append(f1_scores)\n",
    "\n",
    "# Step 5: Aggregate results — median + 95% CI\n",
    "# Pad runs with np.nan to same length\n",
    "max_len = max(len(scores) for scores in all_f1_scores)\n",
    "f1_matrix = np.full((N_runs, max_len), np.nan)\n",
    "for i, scores in enumerate(all_f1_scores):\n",
    "    f1_matrix[i, :len(scores)] = scores\n",
    "\n",
    "# Median & CI\n",
    "median_f1 = np.nanmedian(f1_matrix, axis=0)\n",
    "std_f1 = np.nanstd(f1_matrix, axis=0)\n",
    "ci_95 = 1.57 * std_f1 / np.sqrt(N_runs)  # Approx 95% CI\n",
    "\n",
    "# --- Already computed:\n",
    "# f1_matrix = shape (5, N_iter)\n",
    "# median_f1 = np.nanmedian(f1_matrix, axis=0)\n",
    "\n",
    "# Print Mean F1-score per cycle (across 5 runs)\n",
    "mean_f1 = np.nanmean(f1_matrix, axis=0)\n",
    "\n",
    "print(\"\\n===== Mean F1-score across 5 runs per cycle =====\")\n",
    "for i, f1 in enumerate(mean_f1):\n",
    "    print(f\"Cycle {i+1:02d}, Labeled {(i+1)*batch_size} samples: Mean F1 = {f1:.4f}\")\n",
    "\n",
    "# Step 6: Plot\n",
    "x_axis = np.arange(1, len(median_f1)+1) * batch_size\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_axis, median_f1, marker='o', label='Median F1 (N=5 runs)')\n",
    "plt.fill_between(x_axis, median_f1 - ci_95, median_f1 + ci_95, alpha=0.2, label='~95% CI')\n",
    "plt.title(\"Active Learning (EMC) N=5 runs — Median F1 + 95% CI\")\n",
    "plt.xlabel(\"Number of Labeled Samples\")\n",
    "plt.ylabel(\"Illicit F1-score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffcf2a8-a177-4bec-a5a6-fef7bf886dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d2b0f-2c74-4425-8e9b-aa9fe3670dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Active Learning: IF + Uncertainty Sampling + XGBoost ---\n",
    "# Paper reproduction version: N=5 runs, median F1 + 95% CI\n",
    "\n",
    "# Step 1: Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load labeled data\n",
    "labeled_df = pd.read_csv('elliptic_txs_features_labeled.csv')\n",
    "\n",
    "# Check data\n",
    "print(\"Loaded labeled CSV rows:\", labeled_df.shape[0])\n",
    "print(\"Unique time_steps:\", sorted(labeled_df['time_step'].unique()))\n",
    "print(\"Label distribution:\\n\", labeled_df['class'].value_counts())\n",
    "\n",
    "# Step 3: Split train/test by time_step\n",
    "train_df = labeled_df[labeled_df['time_step'] <= 34].reset_index(drop=True)\n",
    "test_df  = labeled_df[labeled_df['time_step'] >= 35].reset_index(drop=True)\n",
    "\n",
    "# Prepare test set\n",
    "X_test = test_df.drop(columns=['txId', 'time_step', 'class'])\n",
    "y_test = (test_df['class'] == 1).astype(int)\n",
    "\n",
    "# Feature columns\n",
    "feature_columns = [col for col in train_df.columns if col.startswith('feature_') or col.startswith('f')]\n",
    "\n",
    "# Config\n",
    "batch_size = 50\n",
    "max_iterations = 30  # 30 iterations = up to 1500 samples\n",
    "N_runs = 5\n",
    "\n",
    "# Storage for all runs\n",
    "all_f1_scores = []\n",
    "\n",
    "# Step 4: Repeat N=5 runs\n",
    "for run in range(N_runs):\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"=== Run {run+1}/{N_runs}, Random State {run+1}\")\n",
    "    print(\"==========================\")\n",
    "    \n",
    "    # Initialize pools\n",
    "    unlabeled_pool = train_df.copy()\n",
    "    labeled_pool = pd.DataFrame(columns=train_df.columns)\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Warm-up using Isolation Forest\n",
    "    print(\"\\n===== Warm-up stage: Isolation Forest =====\")\n",
    "    iso_forest = IsolationForest(random_state=run+1)\n",
    "    iso_forest.fit(unlabeled_pool[feature_columns])\n",
    "    anomaly_scores = -iso_forest.score_samples(unlabeled_pool[feature_columns])\n",
    "    unlabeled_pool['anomaly_score'] = anomaly_scores\n",
    "    unlabeled_pool = unlabeled_pool.sort_values(by='anomaly_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    found_illicit = False\n",
    "    pointer = 0\n",
    "    warmup_illicit = 0\n",
    "    \n",
    "    while not found_illicit and pointer < len(unlabeled_pool):\n",
    "        batch = unlabeled_pool.iloc[pointer:pointer+batch_size]\n",
    "        labeled_pool = pd.concat([labeled_pool, batch], ignore_index=True)\n",
    "        pointer += batch_size\n",
    "        warmup_illicit = (labeled_pool['class'] == 1).sum()\n",
    "        found_illicit = warmup_illicit > 0\n",
    "    \n",
    "    print(f\"Warm-up selected {len(labeled_pool)} samples.\")\n",
    "    print(f\"Warm-up found {warmup_illicit} illicit samples.\")\n",
    "    \n",
    "    # Remove warm-up samples\n",
    "    unlabeled_pool = unlabeled_pool.iloc[pointer:].drop(columns='anomaly_score').reset_index(drop=True)\n",
    "    \n",
    "    # Active Learning loop\n",
    "    iteration = 1\n",
    "    print(\"\\n===== Active Learning loop: Uncertainty Sampling =====\")\n",
    "    \n",
    "    while len(unlabeled_pool) > 0 and iteration <= max_iterations:\n",
    "        X_train = labeled_pool[feature_columns]\n",
    "        y_train = (labeled_pool['class'] == 1).astype(int)\n",
    "        \n",
    "        xgb_clf = xgb.XGBClassifier(\n",
    "            tree_method='hist',\n",
    "            eval_metric='logloss',\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=run+1\n",
    "        )\n",
    "        \n",
    "        xgb_clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = xgb_clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "        print(f\"[Run {run+1}] XGBoost_Model_{iteration:02d} illicit F1-score: {f1:.4f}\")\n",
    "        \n",
    "        # Uncertainty sampling\n",
    "        X_unlabeled = unlabeled_pool[feature_columns]\n",
    "        proba = xgb_clf.predict_proba(X_unlabeled)[:, 1]\n",
    "        uncertainty = np.abs(proba - 0.5)\n",
    "        \n",
    "        select_idx = np.argsort(uncertainty)[:batch_size]\n",
    "        new_batch = unlabeled_pool.iloc[select_idx]\n",
    "        \n",
    "        labeled_pool = pd.concat([labeled_pool, new_batch], ignore_index=True)\n",
    "        unlabeled_pool = unlabeled_pool.drop(unlabeled_pool.index[select_idx]).reset_index(drop=True)\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    # Save F1-scores of this run\n",
    "    all_f1_scores.append(f1_scores)\n",
    "\n",
    "# Step 5: Aggregate results — median + 95% CI\n",
    "# Pad runs with np.nan to same length\n",
    "max_len = max(len(scores) for scores in all_f1_scores)\n",
    "f1_matrix = np.full((N_runs, max_len), np.nan)\n",
    "for i, scores in enumerate(all_f1_scores):\n",
    "    f1_matrix[i, :len(scores)] = scores\n",
    "\n",
    "\n",
    "# Step 5b: Print Mean F1-score across 5 runs per cycle\n",
    "mean_f1 = np.nanmean(f1_matrix, axis=0)\n",
    "\n",
    "print(\"\\n===== Mean F1-score across 5 runs per cycle =====\")\n",
    "for i, f1 in enumerate(mean_f1):\n",
    "    print(f\"Cycle {i+1:02d}, Labeled {(i+1)*batch_size} samples: Mean F1 = {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Median & CI\n",
    "median_f1 = np.nanmedian(f1_matrix, axis=0)\n",
    "std_f1 = np.nanstd(f1_matrix, axis=0)\n",
    "ci_95 = 1.57 * std_f1 / np.sqrt(N_runs)  # Approx 95% CI\n",
    "\n",
    "# Step 6: Plot\n",
    "x_axis = np.arange(1, len(median_f1)+1) * batch_size\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_axis, median_f1, marker='o', label='Median F1 (N=5 runs)')\n",
    "plt.fill_between(x_axis, median_f1 - ci_95, median_f1 + ci_95, alpha=0.2, label='~95% CI')\n",
    "plt.title(\"Active Learning (Uncertainty Sampling) N=5 runs — Median F1 + 95% CI\")\n",
    "plt.xlabel(\"Number of Labeled Samples\")\n",
    "plt.ylabel(\"Illicit F1-score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b219d84-3f2a-4ea5-bf90-0e154ede27c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571ddad-69a9-458b-bbaf-e1016ce2a399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0134f68-c6e3-4d66-bf28-1e14db852679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a983a-cdb1-4eac-ab41-d1b7fb544b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aaf5ba-bb69-4576-8a99-cf8b4e2fc8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
