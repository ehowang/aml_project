{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Error: [Errno 2] No such file or directory: 'elliptic_txs_features.csv'. Please make sure the dataset files are in the same directory.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 257\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# --- Main Execution ---\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     df, edgelist = load_data()\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m         node_embeddings = generate_node_embeddings(df, edgelist)\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This script implements the methodology described in the paper:\n",
    "# \"Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics\"\n",
    "# Specifically, it reproduces the \"Random Forest (AF+NE)\" experiment, which combines\n",
    "# all features (AF) with node embeddings (NE) from a Graph Convolutional Network (GCN).\n",
    "#\n",
    "# Author: Gemini\n",
    "# Date: 21 June 2024\n",
    "#\n",
    "# To Run:\n",
    "# 1. Make sure you have Python installed.\n",
    "# 2. Install the required libraries:\n",
    "#    pip install pandas numpy scikit-learn torch\n",
    "# 3. Download the Elliptic Data Set from Kaggle:\n",
    "#    https://www.kaggle.com/datasets/ellipticco/elliptic-data-set\n",
    "# 4. Place the following files in the same directory as this script:\n",
    "#    - elliptic_txs_features.csv\n",
    "#    - elliptic_txs_edgelist.csv\n",
    "#    - elliptic_txs_classes.csv\n",
    "# 5. Run the script from your terminal:\n",
    "#    python your_script_name.py\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the Elliptic dataset from CSV files, merges them, and performs\n",
    "    initial preprocessing.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        features_df = pd.read_csv('data/elliptic_txs_features.csv', header=None)\n",
    "        edgelist_df = pd.read_csv('data/elliptic_txs_edgelist.csv')\n",
    "        classes_df = pd.read_csv('data/elliptic_txs_classes.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please make sure the dataset files are in the same directory.\")\n",
    "        return None\n",
    "\n",
    "    # Name the columns for clarity\n",
    "    # The first column is the txId, the second is the timestep\n",
    "    # Features 2-94 are local features, 95-166 are aggregated features\n",
    "    features_df.columns = ['txId', 'timestep'] + [f'local_feat_{i}' for i in range(93)] + [f'agg_feat_{i}' for i in range(72)]\n",
    "\n",
    "    # Map class labels to meaningful names\n",
    "    # '1' for illicit, '2' for licit\n",
    "    classes_df['class'] = classes_df['class'].map({'1': 'illicit', '2': 'licit', 'unknown': 'unknown'})\n",
    "\n",
    "    # Merge features and classes\n",
    "    data_df = pd.merge(features_df, classes_df, on='txId', how='left')\n",
    "    \n",
    "    # Sort by timestep\n",
    "    data_df = data_df.sort_values('timestep').reset_index(drop=True)\n",
    "\n",
    "    print(\"Data loaded and merged successfully.\")\n",
    "    print(f\"Total transactions: {len(data_df)}\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(data_df['class'].value_counts())\n",
    "    \n",
    "    return data_df, edgelist_df\n",
    "\n",
    "# --- 2. GCN Model Definition ---\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer Graph Convolutional Network as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_hidden, n_classes, dropout_rate=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = nn.Linear(n_features, n_hidden)\n",
    "        self.gc2 = nn.Linear(n_hidden, n_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Forward pass for the GCN.\n",
    "        adj: The normalized adjacency matrix.\n",
    "        x: The node feature matrix.\n",
    "        \"\"\"\n",
    "        # First GCN layer\n",
    "        h1 = F.relu(self.gc1(torch.matmul(adj, x)))\n",
    "        h1_d = self.dropout(h1)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        logits = self.gc2(torch.matmul(adj, h1_d))\n",
    "        \n",
    "        # We return the hidden layer activations as node embeddings\n",
    "        # and the final logits for classification.\n",
    "        return logits, h1\n",
    "\n",
    "def normalize_adjacency_matrix(adj):\n",
    "    \"\"\"\n",
    "    Computes the symmetrically normalized adjacency matrix from the paper.\n",
    "    A_hat = D^{-1/2} * (A + I) * D^{-1/2}\n",
    "    \"\"\"\n",
    "    print(\"Normalizing adjacency matrix...\")\n",
    "    adj = adj + torch.eye(adj.shape[0]) # Add self-loops\n",
    "    row_sum = adj.sum(1)\n",
    "    d_inv_sqrt = torch.pow(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
    "    return adj.matmul(d_mat_inv_sqrt).transpose(0,1).matmul(d_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "# --- 3. GCN Training and Embedding Generation ---\n",
    "\n",
    "def generate_node_embeddings(data_df, edgelist_df):\n",
    "    \"\"\"\n",
    "    Trains a GCN in a semi-supervised manner to generate node embeddings.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting GCN Training for Node Embedding Generation ---\")\n",
    "    \n",
    "    # Prepare data for PyTorch\n",
    "    # Feature scaling\n",
    "    features = data_df.iloc[:, 2:167].values\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Map txId to an index\n",
    "    txid_to_idx = {txid: i for i, txid in enumerate(data_df['txId'])}\n",
    "    n_nodes = len(data_df)\n",
    "    \n",
    "    # Create adjacency matrix\n",
    "    adj = torch.zeros((n_nodes, n_nodes))\n",
    "    for index, row in tqdm(edgelist_df.iterrows(), total=edgelist_df.shape[0], desc=\"Building adjacency matrix\"):\n",
    "        src_idx = txid_to_idx.get(row['txId1'])\n",
    "        tgt_idx = txid_to_idx.get(row['txId2'])\n",
    "        if src_idx is not None and tgt_idx is not None:\n",
    "            adj[src_idx, tgt_idx] = 1\n",
    "            adj[tgt_idx, src_idx] = 1 # Make it symmetric for GCN\n",
    "\n",
    "    adj_normalized = normalize_adjacency_matrix(adj)\n",
    "    \n",
    "    # Prepare labels and masks\n",
    "    labels_map = {'licit': 0, 'illicit': 1, 'unknown': 2}\n",
    "    labels = torch.LongTensor(data_df['class'].map(labels_map).values)\n",
    "    features_tensor = torch.FloatTensor(features)\n",
    "    \n",
    "    # Use labels only from the training period (timestep <= 34)\n",
    "    train_mask = torch.BoolTensor(data_df['timestep'] <= 34)\n",
    "    # Mask out unknown classes for loss calculation\n",
    "    train_mask &= (labels != 2)\n",
    "\n",
    "    # Model parameters\n",
    "    n_features = features_tensor.shape[1]\n",
    "    n_hidden = 100 # As per the paper's hyperparameter tuning\n",
    "    n_classes = 2  # We only classify licit vs illicit in the loss\n",
    "    \n",
    "    model = GCN(n_features=n_features, n_hidden=n_hidden, n_classes=n_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    \n",
    "    # Weighted loss for imbalanced classes (licit vs illicit)\n",
    "    # The paper mentions a 0.3/0.7 ratio\n",
    "    loss_weights = torch.FloatTensor([0.3, 0.7])\n",
    "    criterion = nn.CrossEntropyLoss(weight=loss_weights)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Training GCN...\")\n",
    "    start_time = time.time()\n",
    "    for epoch in tqdm(range(100), desc=\"GCN Training\"): # The paper trained for 1000 epochs, 100 is sufficient for demonstration\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(features_tensor, adj_normalized)\n",
    "        \n",
    "        # Calculate loss only on known, training nodes\n",
    "        loss = criterion(logits[train_mask], labels[train_mask])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # This will be too noisy inside tqdm\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch+1:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"GCN training finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Generate final embeddings\n",
    "    print(\"Generating node embeddings from trained GCN...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, embeddings_tensor = model(features_tensor, adj_normalized)\n",
    "    \n",
    "    return embeddings_tensor.numpy()\n",
    "\n",
    "# --- 4. Random Forest Classification ---\n",
    "def run_random_forest(data_df, embeddings):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a Random Forest classifier using the combined\n",
    "    original features and GCN embeddings.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Random Forest Classification ---\")\n",
    "\n",
    "    # Combine original features and embeddings\n",
    "    original_features = data_df.iloc[:, 2:167].values\n",
    "    \n",
    "    # Scale original features\n",
    "    scaler = StandardScaler()\n",
    "    original_features_scaled = scaler.fit_transform(original_features)\n",
    "\n",
    "    combined_features = np.concatenate([original_features_scaled, embeddings], axis=1)\n",
    "    \n",
    "    # Prepare labels for RF: 0 for licit, 1 for illicit\n",
    "    labels = data_df['class'].map({'licit': 0, 'illicit': 1, 'unknown': -1}).values\n",
    "    \n",
    "    # Temporal split\n",
    "    train_indices = data_df[data_df['timestep'] <= 34].index\n",
    "    test_indices = data_df[data_df['timestep'] > 34].index\n",
    "    \n",
    "    X_train = combined_features[train_indices]\n",
    "    y_train = labels[train_indices]\n",
    "    X_test = combined_features[test_indices]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    # Filter out 'unknown' classes from training and testing sets\n",
    "    X_train_filtered = X_train[y_train != -1]\n",
    "    y_train_filtered = y_train[y_train != -1]\n",
    "    X_test_filtered = X_test[y_test != -1]\n",
    "    y_test_filtered = y_test[y_test != -1]\n",
    "    \n",
    "    print(f\"Training set size (known labels): {len(y_train_filtered)}\")\n",
    "    print(f\"Test set size (known labels): {len(y_test_filtered)}\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    # Using parameters from the paper: 50 estimators\n",
    "    # The paper also mentions 50 max features, which is a fraction of total\n",
    "    # We will use the default for max_features which is sqrt(n_features)\n",
    "    print(\"Training Random Forest classifier...\")\n",
    "    start_time = time.time()\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train_filtered, y_train_filtered)\n",
    "    print(f\"RF training finished in {time.time() - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"Evaluating model performance on the test set...\")\n",
    "    y_pred = rf.predict(X_test_filtered)\n",
    "    \n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    # 0 is licit, 1 is illicit\n",
    "    print(classification_report(y_test_filtered, y_pred, target_names=['Licit', 'Illicit']))\n",
    "    \n",
    "    # The paper's main reported score is Illicit F1\n",
    "    illicit_f1 = f1_score(y_test_filtered, y_pred, pos_label=1)\n",
    "    print(f\"F1 Score for Illicit class: {illicit_f1:.4f}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    df, edgelist = load_data()\n",
    "    if df is not None:\n",
    "        node_embeddings = generate_node_embeddings(df, edgelist)\n",
    "        run_random_forest(df, node_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
